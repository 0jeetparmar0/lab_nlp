{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "rKzSKuaRmWIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS TAG"
      ],
      "metadata": {
        "id": "VL0QukWa5wKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS TAGGING\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "sent= \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens=word_tokenize(sent)\n",
        "pos_tags=nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6SONQoM5lRl",
        "outputId": "932d3a42-d4fd-4f8b-ecbf-934be102f04b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER"
      ],
      "metadata": {
        "id": "GdG5OBKS4wwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag,ne_chunk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "sent= \"Barack Obama was born in Hawaii in 1961 and was the president of the United States.\"\n",
        "tokens=word_tokenize(sent)\n",
        "pos_tags = pos_tag(tokens)\n",
        "named_entities=ne_chunk(pos_tags)\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptSbGLOF4jTF",
        "outputId": "90500dca-e43a-4dd5-80ca-af1b64cd9925"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Hawaii/NNP)\n",
            "  in/IN\n",
            "  1961/CD\n",
            "  and/CC\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  president/NN\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token"
      ],
      "metadata": {
        "id": "lkSJeK6xiae0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t=\"\"\"Hello Welcome,to NLP Tutorials.\n",
        "Please do watch the entire tutorial to become expert in NLP.\n",
        "\"\"\"\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "print(t)\n",
        "nltk.download('punkt')\n",
        "documents=sent_tokenize(t)\n",
        "type(documents)\n",
        "for sentence in documents:\n",
        "    print(sentence)\n",
        "word_tokenize(t)\n",
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))\n",
        "wordpunct_tokenize(t)\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlvg5cq-nSFd",
        "outputId": "6e369dcd-8dd0-43e8-d39d-afee12fa354d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to NLP Tutorials.\n",
            "Please do watch the entire tutorial to become expert in NLP.\n",
            "\n",
            "Hello Welcome,to NLP Tutorials.\n",
            "Please do watch the entire tutorial to become expert in NLP.\n",
            "['Hello', 'Welcome', ',', 'to', 'NLP', 'Tutorials', '.']\n",
            "['Please', 'do', 'watch', 'the', 'entire', 'tutorial', 'to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'NLP',\n",
              " 'Tutorials.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'tutorial',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "lCFakfRmoOBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "WaGON416oPY8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "porter steeming"
      ],
      "metadata": {
        "id": "FyJAAT1JoVJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming=PorterStemmer()\n",
        "for word in words:\n",
        "    print(word+\"---->\"+stemming.stem(word))\n",
        "stemming.stem('congratulations')\n",
        "stemming.stem(\"sitting\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "UPLFoi48oXB8",
        "outputId": "7cf6a594-7b1d-4b91-f39f-36326cfede12"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegexpStemmer class"
      ],
      "metadata": {
        "id": "LWwDof32oj0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "reg_stemmer.stem('eating')\n",
        "for word in words:\n",
        "    print(word+\"---->\"+reg_stemmer.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYEgN76voh8d",
        "outputId": "885bf08b-032a-4421-cd52-584cda5deed3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->writ\n",
            "writes---->write\n",
            "programming---->programm\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball Stemmer"
      ],
      "metadata": {
        "id": "k4JM33NSpDqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballsstemmer=SnowballStemmer('english')\n",
        "for word in words:\n",
        "    print(word+\"---->\"+snowballsstemmer.stem(word))\n",
        "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")\n",
        "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")\n",
        "snowballsstemmer.stem('goes')\n",
        "stemming.stem('goes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "pGPOSve1pEWk",
        "outputId": "a23bf784-454e-4b84-fe81-2020d41b5377"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stopword"
      ],
      "metadata": {
        "id": "sodatZfmuDv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "sentence = \"This is a simple example showing how to remove stop words in NLP.\"\n",
        "words = word_tokenize(sentence)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Filtered Sentence:\", ' '.join(filtered_sentence))"
      ],
      "metadata": {
        "id": "tyy0iCB2yAbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd14086c-2c89-4a65-babb-73026b35f8be"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: This is a simple example showing how to remove stop words in NLP.\n",
            "Filtered Sentence: simple example showing remove stop words NLP .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatizers"
      ],
      "metadata": {
        "id": "080yWjbUvx1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoRHhYQ8NsmJ",
        "outputId": "b011a04b-5d25-4fc2-ad55-214a9c2c0b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "## Q&A,chatbots,text summarization\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))\n",
        "lemmatizer.lemmatize(\"goes\",pos='v')\n",
        "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\")"
      ]
    }
  ]
}